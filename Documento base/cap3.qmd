---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Modelos mixtos

```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
knitr::opts_chunk$set(
  fig.width = 6, 
  fig.height = 4, 
  dev = "png",
  fig.align = "center"
)
theme_set(theme_minimal(base_size = 12))
```

En este capítulo, exploraremos los **Modelos Lineales Mixtos (LMM)**, los **Modelos Lineales Generalizados (GLM)** y los **Modelos Lineales Generalizados Mixtos (GLMM)**, tres métodos estadísticos fundamentales para el análisis de datos longitudinales. Veremos cómo los LMM permiten modelar la variabilidad entre individuos a través de efectos aleatorios y fijos, facilitando el estudio de la correlación entre medidas repetidas. A continuación, introduciremos los GLM, que extienden la regresión lineal para manejar variables respuesta que no siguen una distribución normal, utilizando funciones de enlace y distribuciones de la familia exponencial. Finalmente, presentaremos los GLMM, que combinan las fortalezas de los modelos mixtos y los GLM para analizar datos longitudinales que no siguen una distribución normal incorporando tanto efectos aleatorios como funciones de enlace apropiadas. A lo largo del capítulo, revisaremos sus formulaciones matemáticas, sus hipótesis clave y cómo validarlas en la práctica.

## Comparación de modelos con efectos fijos, aleatorios y mixtos

Para ilustrar estos modelos, comenzaremos con un ejemplo práctico aplicado al conjunto de datos Orthodont del paquete nlme de R [@R-nlme], donde analizaremos la evolución de la distancia entre los dientes (`distance`) en función de la edad (`age`) en diferentes sujetos. Sus variables principales son:

-   `distance`: distancia entre los dientes (variable respuesta).

-   `age`: edad del niño (variable predictora principal).

-   `Subject`: identificador del niño (variable de agrupación para efectos aleatorios).

En las siguientes secciones, compararemos tres enfoques distintos:

-   Modelo con sólo **efectos fijos**: se asume que todos los sujetos siguen la misma relación.

-   Modelo mixto con **intercepto aleatorio**: este modelo incluye un intercepto específico para cada individuo, permitiendo que cada sujeto tenga un valor inicial propio. Sin embargo, la pendiente que define la evolución temporal es la misma para todos los individuos. Aquí, como ya incorporamos efectos aleatorios, hablamos de un modelo lineal mixto.

-   Modelo mixto con **intercepto y pendiente aleatoria**: es un modelo que incluye tanto efectos fijos como efectos aleatorios. Los efectos fijos representan patrones y tendencias globales comunes a toda la población, mientras que los efectos aleatorios modelan la variabilidad individual. En este tipo de modelos podemos introducir interceptos aleatorios, pendientes aleatorias o ambos, dependiendo de la estructura de los datos. Este método es muy útil en datos longitudinales, donde las observaciones están agrupadas por individuo y tienen una dependencia entre ellas.

### Modelo con efectos fijos

El primer modelo que consideramos es una regresión lineal simple, en la que asumimos que la distancia interdental (`distance`) varía en función de la edad (`age`), asumiendo que todas las observaciones son independientes e ignorando su estructura jerárquica (medidas repetidas por individuo). La ecuación del modelo es: $$
distance_i = \beta_0 + \beta_1 age_i + \epsilon_i$$ Aquí, $distance_i$ es la distancia interdental de la observación $i$, donde $i$ va desde 1 hasta $n$, con $n$ siendo el tamaño total de la muestra; $\beta_0$ es el intercepto común a todos los sujetos, $\beta_1$ es la pendiente (cómo cambia la distancia con la edad), y $\epsilon_i$ es el error aleatorio.

```{r include=FALSE, warning=FALSE, message=FALSE}
# Cargar paquetes
library(nlme)
library(lme4)

# Cargar datos longitudinales
data(Orthodont, package = "nlme")

# Ajustar modelo de regresión lineal simple 
modelo_fijo <- lm(distance ~ age, data = Orthodont)

# Resumen del modelo
r2 <- summary(modelo_fijo)$r.squared
```

```{r}
#| label: fig-reglinsimp
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Modelo con efectos fijos: Regresión Lineal Simple."
#| fig-width: 5
#| fig-height: 3.5
library(ggplot2)
ggplot(Orthodont, aes(x = age, y = distance)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Modelo con efectos fijos: Regresión Lineal Simple",
       x = "Edad", y = "Distancia interdental")

```

Este modelo de la @fig-reglinsimp considera únicamente la edad (`age`) como predictor de la distancia (`distance`) y, como bien mencionamos antes, no tiene en cuenta que los datos son medidas repetidas de los mismos individuos, lo que puede llevar a errores de estimación debido a que se ignora por completo la correlación entre observaciones de un mismo sujeto. Como podemos comprobar a través de este ejemplo, si se ignora la estructura jerárquica, produciríamos estimaciones erróneas de la variabilidad en la población, obteniendo un coficiente de determinación R² muy bajo (**`r round(r2, 3)`**).

### Modelo con intercepto aleatorio

Ahora ajustamos un modelo con efectos aleatorios, en el que permitimos que cada niño tenga su propio intercepto aleatorio ($u_i$), capturando de esta forma la variabilidad entre individuos. La ecuación del modelo es: $$
distance_{ij} = \beta_0 + u_i +\beta_1 age_{ij} + \epsilon_{ij}$$ donde:

-   $distance_{ij}$ representa la distancia observada para el niño $i$ en su $j$-ésima medida.

-   $age_{ij}$ es la edad correspondiente a esa misma observación.

-   $\beta_0$ es el intercepto poblacional, común a todos los individuos.

-   $u_i$ es el efecto aleatorio del sujeto $i$, que permite que cada niño tenga un intercepto diferente.

-   $\beta_1$ es la pendiente común, que modela el cambio medio de la distancia con respecto a la edad.

-   $\epsilon_{ij}$ es el error aleatorio, que representa la variabilidad residual no explicada por el modelo.

El subíndice $i$ recorre los individuos (niños), mientras que $j$ recorre las diferentes observaciones para cada individuo. De esta forma, $j = 1, \dots, n_i$, siendo $n_i$ el número de medidas realizadas para el niño $i$.

```{r include=FALSE, warning=FALSE, message=FALSE}
# Cargar paquetes
library(nlme)
library(lme4)

# Cargar datos longitudinales
data(Orthodont, package = "nlme")

modelo_aleatorio <- lmer(distance ~ age + (1 | Subject), data = Orthodont)
summary(modelo_aleatorio)


# Resumen del modelo
r2 <- summary(modelo_aleatorio)$r.squared
# Extraer la varianza del intercepto por sujeto
var_intercept <- as.numeric(VarCorr(modelo_aleatorio)$Subject[1])
# Extraer la varianza residual
var_residual <- attr(VarCorr(modelo_aleatorio), "sc")^2

```

```{r}
#| label: fig-efectos-aleatorios
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Modelo con efectos aleatorios a través de intercepto aleatorio."
#| fig-width: 6
#| fig-height: 3.7

library(ggplot2)
library(lme4)

# Crear data frame con intercepto y pendiente para cada sujeto 
interceptos <- ranef(modelo_aleatorio)$Subject[, 1] + fixef(modelo_aleatorio)[1]
pendiente <- fixef(modelo_aleatorio)[2]
sujetos <- rownames(ranef(modelo_aleatorio)$Subject)

df_lineas <- data.frame(
  Subject = sujetos,
  intercepto = interceptos,
  pendiente = pendiente
)

# Gráfico
ggplot(Orthodont, aes(x = age, y = distance, group = Subject, color = Subject)) +
  geom_line(linetype = "dashed") +  # Observaciones por individuo
  geom_abline(data = df_lineas, aes(intercept = intercepto, slope = pendiente, color = Subject), alpha = 0.5) +
  labs(
    title = "Modelo con intercepto aleatorio",
    x = "Edad", y = "Distancia interdental"
  ) +
  theme_minimal() 


```

Como podemos apreciar en la @fig-efectos-aleatorios, ahora tenemos un término que asocia a cada individuo (`Subject`) su propio intercepto aleatorio, permitiendo que la relación entre la distancia y la edad varíe entre individuos en lugar de asumir un único intercepto para todos como hacíamos en la @fig-reglinsimp. Esto significa que algunos sujetos pueden tener distintos valores iniciales de `distance` sin afectar a la tendencia general de la población. La principal diferencia de los efectos aleatorios la podemos apreciar en la variabilidad del modelo, ya que tenemos una varianza del intercepto por sujeto de **`r round(var_intercept, 3)`** y una varianza residual de **`r round(var_residual, 3)`**, lo que significa que aunque cada sujeto tiene un valor inicial diferente, todavía hay una parte de la variabilidad del modelo que no se explica por los efectos fijos ni por las diferencias entre sujetos.

Este modelo, al incorporar un intercepto aleatorio específico para cada niño, permite capturar la variabilidad individual no explicada por la edad.

### Modelo mixto

Finalmente, ajustamos un **Modelo Lineal Mixto (LMM)** en el que consideramos tanto efectos fijos como aleatorios. Permitimos que cada niño tenga su propio intercepto ($u_i$) y pendiente ($v_i$) aleatorios, permitiendo que la relación entre edad y distancia interdental varíe entre individuos. La ecuación del modelo es: $$
distance_{ij} = \beta_0 + u_i + (\beta_1 + v_i) age_{ij} + \epsilon_{ij}$$ Aquí $u_i$ es el intercepto específico de cada sujeto, y $v_i$ permite que la pendiente también varíe por individuo.

```{r include=FALSE, warning=FALSE, message=FALSE}
# Cargar paquetes
library(nlme)
library(lme4)

# Cargar datos longitudinales
data(Orthodont, package = "nlme")

modelo_mixto <- lmer(distance ~ age + (1 + age | Subject), data = Orthodont)
summary(modelo_mixto)


# Resumen del modelo
r2 <- summary(modelo_mixto)$r.squared
# Extraer la varianza del intercepto por sujeto
var_intercept_completo <- as.numeric(VarCorr(modelo_mixto)$Subject[1])
var_pendiente_completo <- as.numeric(VarCorr(modelo_mixto)$Subject["age", "age"])
# Extraer la varianza residual
var_residual_completo <- attr(VarCorr(modelo_mixto), "sc")^2
```

```{r}
#| label: fig-modelo-lmm
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Modelo Lineal Mixto (LMM con efectos fijos y aleatorios)."
#| fig-width: 6
#| fig-height: 3.7
# Obtener coeficientes por individuo y añadir columna Subject
coefs <- coef(modelo_mixto)$Subject
coefs$Subject <- rownames(coefs)

# Gráfico con pendientes coloreadas por individuo
ggplot(Orthodont, aes(x = age, y = distance, group = Subject, color = Subject)) +
  geom_line(linetype = "dashed") +  # Observaciones por individuo
  geom_abline(data = coefs, aes(intercept = `(Intercept)`, slope = age, color = Subject), 
              size = 0.6, alpha = 0.5) +
  labs(title = "Modelo mixto (intercepto y pendiente aleatoria)", 
       x = "Edad", y = "Distancia interdental") +
  theme_minimal()


```

Ahora, al contar con interceptos aleatorios y permitir que la pendiente varíe entre sujetos, cada sujeto puede tener una tasa de crecimiento diferente en la distancia dental a lo largo del tiempo. Observando el modelo de la @fig-modelo-lmm, vemos que no sólo los interceptos cambian, sino que las pendientes son ligeramente distintas. Además, hemos reducido la varianza residual a **`r round(var_residual_completo, 3)`**, y ahora contamos con una varianza del intercepto por sujeto de **`r round(var_intercept_completo, 3)`** y una variación de la pendiente entre sujetos de **`r round(var_pendiente_completo, 3)`**. Este tipo de modelos es más realista cuando hay variabilidad individual en la evolución de la variable respuesta, además de ser más flexible ya que permite que tanto el intercepto como la pendiente varíen entre individuos.

Este último modelo generaliza la idea que vimos en el capítulo anterior, donde ajustábamos una regresión por individuo (@fig-modelo-individual). En aquel caso, teníamos una pendiente e intercepto diferentes por persona, pero ajustados de forma separada. Los modelos mixtos permiten hacer esto mismo pero de forma conjunta y efectiva, combinando la información de todos los individuos para obtener mejores estimaciones sin tener que ajustar un modelo por separado para cada uno.

Si comparamos los 3 modelos, podemos observar que el modelo con solo efectos fijos asume una única relación entre edad y distancia interdental, ignorando la variabilidad entre individuos. El modelo con intercepto aleatorio permite que cada sujeto tenga su propio intercepto, pero mantiene una pendiente común para todos. El modelo mixto (LMM) final es el más completo, permitiendo que tanto el intercepto como la pendiente varíen entre individuos. Esto demuestra la importancia de los Modelos Lineales Mixtos en el análisis de datos longitudinales, ya que incorporan tanto la variabilidad individual como la estructura jerárquica de los datos.

Antes de hablar sobre los Modelos Lineales Mixtos (LMM), conviene recordar que, en el ejemplo anterior, analizamos cómo una variable dependiente (la distancia interdental) se explicaba a partir de una única variable independiente (la edad). Sin embargo, tanto en la regresión lineal clásica como en los modelos lineales mixtos, es posible incorporar múltiples variables explicativas que ayuden a modelar mejor la respuesta. A continuación, veremos cómo los LMM permiten esta flexibilidad, además de captar la variabilidad entre individuos a través de efectos aleatorios.

## Modelos Lineales Mixtos (LMM)

Los Modelos Lineales Mixtos (LMM) son métodos estadísticos que permiten analizar datos longitudinales cuando la variable respuesta sigue una distribución normal. Uno de sus aspectos más característicos, según Francisco Hernández-Barrera en su libro *Modelos mixtos con R* [@modelos_mixtos], es que asumen una relación directa entre el vector de observaciones y las covariables. Esta técnica resulta especialmente eficaz ya que permite introducir efectos aleatorios y determinar la estructura de correlación entre medidas repetidas del mismo sujeto. Además, estos modelos son robustos frente a la presencia de datos faltantes, convirtiéndolos en un método muy flexible. Una de sus principales ventajas es que permiten modelar tanto la variabilidad entre individuos, a través de efectos aleatorios, como la correlación de sus observaciones; incluyendo covariables tanto a nivel individual como grupal, y respetando la dependencia entre observaciones.

Según Julian Faraway en *Extending the Linear Model with R* [@faraway], un efecto fijo es una constante desconocida que intentamos estimar a partir de los datos, mientras que un efecto aleatorio es una variable aleatoria que refleja variación individual no explicada por covariables; una distinción fundamental para interpretar correctamente los modelos. Otra de las ventajas de este tipo de modelos es su capacidad para generalizar estructuras de datos complejas, lo que hace recomendable su uso con datos longitudinales. No obstante, debemos tener en cuenta que el número de efectos aleatorios que se pueden incluir está limitado: no deben superar el número de observaciones por individuo, una restricción importante a la hora de ajustar modelos reales. En resumen, los LMM permiten modelar al mismo tiempo los efectos comunes a toda la población (efectos fijos) y la variabilidad individual (efectos aleatorios), proporcionando estimaciones más precisas y respetando la estructura de dependencia de los datos longitudinales.

La ecuación para este tipo de modelos, en los que $y_{ij}$ representa la observación $j$-ésima del individuo $i$: $$
y_{ij} = \beta_0 + \sum_{k=1}^{K} \beta_k x_{ijk} + u_{0i} + \sum_{k=1}^{K} u_{ki} x_{ijk} + e_{ij}
$$

-   $i = 1, \dots, N$: indica el individuo o sujeto.

-   $j = 1, \dots, n_i$: indica la observación o medición $j$-ésima del individuo $i$. Cada sujeto puede tener un número diferente de observaciones.

-   $k = 1, \dots, K$: indica las variables explicativas o predictoras incluidas en el modelo, siendo $K$ el número total de variables independientes.

-   $\beta_0$ y $\beta_k$ son los efectos fijos (intercepto y pendientes comunes a todos los individuos). En conjunto, modelan el efecto medio de los predictores sobre la respuesta.

-   $u_{0i}$ y $u_{ki}$ son los efectos aleatorios (variaciones individuales del intercepto y de las pendientes), los cuales se asumen que siguen una distribución normal de media cero. Cada individuo $i$ puede tener una pendiente propia para cada predictor $x_{ijk}$, lo que permite capturar variaciones individuales en la relación entre las variables explicativas y la variable respuesta.

-   $x_{ijk}$ son las variables explicativas.

-   $e_{ij}$ es el término de error residual, que recoge la variación no explicada por el modelo.

En la práctica, este modelo se especifica en R mediante la función `lmer()` del paquete `lme4` [@lme4].

Una vez formulado el modelo, necesitamos estimar los parámetros, contando con dos métodos principales de estimación como bien se explica en *Modelos mixtos con R* [@modelos_mixtos]. El primero es el método de máxima verosimilitud (ML), que utiliza la función de verosimilitud completa del modelo. Este método estima tanto los efectos fijos como las varianzas de los efectos aleatorios, y es útil para comparar modelos con diferentes efectos fijos. El otro método es el de máxima verosimilitud restringida (REML), que estima solo las varianzas de los efectos aleatorios, ajustando los grados de libertad para evitar sesgo en la estimación de la varianza. El REML es el método preferido para comparar modelos con la misma estructura de efectos fijos, pero distinta estructura de efectos aleatorios.

Cuando ya se ha ajustado el modelo, el siguiente paso es seleccionar la estructura más adecuada de efectos fijos y aleatorios. La selección del modelo es clave en el análisis de datos longitudinales, ya que el hecho de incluir o descartar ciertos efectos puede afectar notablemente la calidad del ajuste y la interpretación. En la práctica, se recomienda seguir una estrategia jerárquica, comenzando por un modelo completo que incluya todas las variables candidatas como efectos fijos, así como una estructura lo más completa posible de efectos aleatorios. Para comparar modelos con distinta estructura de efectos fijos, debe utilizarse el método de máxima verosimilitud (ML), debido a que la función de verosimilitud incorpora estos efectos y permite su comparación. Una vez seleccionada la mejor combinación de efectos fijos, se recomienda ajustar el modelo final con máxima verosimilitud restringida (REML), que proporciona estimaciones más precisas de los componentes de varianza (efectos aleatorios).

Además de comparar modelos mediante ML o REML, se utilizan criterios de información como el AIC (Akaike Information Criterion) o el BIC (Bayesian Information Criterion). Ambos equilibran el ajuste del modelo con su complejidad, llegando a penalizar la inclusión de demasiados parámetros. En particular, el BIC tiende a ser más conservador que el AIC al penalizar más los modelos complejos.

Por último, es esencial validar el modelo ajustado. Esto se realiza evaluando las asunciones sobre los residuos del modelo, tal como se hace en la regresión clásica. En concreto, un gráfico de residuos estandarizados vs valores predichos debe mostrar una nube de puntos sin estructura aparente, lo que indica homocedasticidad; y en el QQ-plot, que permite evaluar la normalidad de los residuos, los puntos deben seguir aproximadamente la diagonal si queremos asumir normalidad. Aunque algunos métodos para validar modelos mixtos utilizan los Empirical Bayes Estimates (EBEs) para evaluar la distribución de los efectos aleatorios, en este trabajo optamos por utilizar el paquete `DHARMa` [@dharma], que ofrece una validación más sistemática de los supuestos del modelo a través de simulaciones de residuos. A diferencia de los EBEs, que pueden inducir sesgos al tratarse de estimaciones condicionadas, `DHARMa` genera residuos simulados independientes de la estructura del modelo, permitiendo detectar problemas como falta de normalidad, heterocedasticidad o dependencia estructural. Este método resulta especialmente apropiado cuando se trabaja con estructuras de datos complejas, como en nuestro caso con datos longitudinales.

## Modelos Lineales Generalizados (GLM)

En la sección anterior trabajamos con modelos lineales mixtos (LMM), los cuales asumen que la variable respuesta sigue una distribución normal, que la relación entre los predictores y la respuesta es lineal y que los residuos presentan varianza constante. Sin embargo, en muchas situaciones reales estas asunciones no se cumplen, ya que la variable de interés puede ser binaria o puede no modelarse adecuadamente con una distribución normal. Ante este tipo de limitaciones, resulta necesario generalizar el modelo para poder adaptarlo a otras distribuciones y a relaciones no lineales entre predictores y respuesta. Para ello, introduciremos los Modelos Lineales Generalizados (GLM), que permiten modelar variables respuesta de distintas familias de distribuciones manteniendo una estructura lineal en los predictores, a través de funciones de enlace. Los Modelos Lineales Generalizados resultan clave para desarrollar modelos mixtos aún más generales y aplicables a una mayor variedad de contextos.

Los Modelos Lineales Generalizados son una generalización de los modelos lineales para una variable respuesta perteneciente a la familia exponencial, en la que tenemos una función de enlace que describe cómo la media de la variable respuesta y la combinación lineal de variables explicativas están relacionadas. Los GLM son una clase de modelos más amplia que tienen formas parecidas para sus varianzas y verosimilitudes, generalizando la regresión lineal múltiple [@roback_legler].

La familia exponencial tiene esta forma: $$
f(y \mid \theta, \phi) = \exp \left[ \frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi) \right].
$$ En esta ecuación, $\theta$ es el **parámetro canónico** y representa la posición; mientras que $\phi$ es el **parámetro de dispersión** y representa la escala. De la misma forma, $a$, $b$ y $c$ representan diferentes miembros de la familia exponencial. En función del parámetro de dispersión, podemos distinguir entre familias exponenciales de un parámetro, y familias exponenciales de dos parámetros.

Como familias exponenciales de un parámetro, tenemos las distribuciones de Poisson y la Binomial. Vamos a demostrar que la distribución de Poisson es, en efecto, una familia exponencial de un parámetro.

Para ello, aplicando propiedades logarítmicas, podemos definir la distribución de Poisson como: $$
P(Y = y) = e^{-\lambda} e^{y \log \lambda} e^{-\log(y!)} \\
= e^{y \log \lambda - \lambda - \log(y!)}
$$ Si comparamos esta función de masa de probabilidad con la función de probabilidad general para familias con un único parámetro, podemos ver que: $$
\begin{aligned}
a(y) &= y \\
b(\theta) &= \log(\lambda) \\
c(\theta) &= -\lambda \\
d(y) &= -\log(y!)
\end{aligned}
$$ La función $b(\theta)$ es lo que denominamos **enlace canónico**, una función que nos permite modelar como una función lineal de variables explicativas.

Como familias exponenciales de dos parámetros, tenemos la distribución gamma y la normal. De forma parecida a la anterior, demostraremos que la distribución normal es una familia exponencial de dos parámetros.

Podemos definir la función de densidad de una distribución normal como: $$
f(y | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
$$ Si separamos términos y los escribimos como términos logarítmicos, tenemos que: $$
f(y | \mu, \sigma^2) = \exp \left( y \cdot \frac{\mu}{\sigma^2} - \frac{y^2}{2\sigma^2} + \left( -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2) \right) \right)
$$ Si comparamos esta función de densidad con la forma general de la familia exponencial, podemos ver que: $$
\begin{aligned}
a(y) = y\\
b(\mu, \sigma^2) = \frac{\mu}{\sigma^2}\\
c(\mu, \sigma^2) = -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\\
d(y, \sigma^2) = -\frac{y^2}{2\sigma^2}\\
\end{aligned}
$$ Por lo tanto, demostramos que la distribución normal también pertenece a la familia exponencial, pero con una peculiaridad respecto a la distribución de Poisson: es una familia exponencial de dos parámetros, la media $\mu$ y la varianza $\sigma^2$. En este caso, el término $b(\mu, \sigma^2)$ es el enlace canónico que conecta las variables explicativas con el modelo.

En concreto, para los casos en los que la respuesta no es normal, la ecuación del modelo es la siguiente: $$
g\left(E(y_{ij} \mid x_{ijk}, \beta_{0i}, \dots, \beta_{Ki})\right) = \beta_{0i} + \sum_{k=1}^{K} \beta_{ki}x_{ijk}
$$ Donde g es la función enlace y, pese a que puede parecerse mucho a la función para modelos LMM, tienen algunas diferencias, como que en el primer miembro tenemos el enlace del valor esperado en vez de la variable respuesta, y en el segundo miembro no se cuenta con los errores; por lo que no existe una matriz de correlaciones de los residuos. De esta forma, ya hemos generalizado nuestro modelo para manejar variables respuesta que no siguen una distribución normal. A través de esta generalización, podemos escribir la función de masa o densidad de probabilidad de distintas distribuciones para poder modelar el enlace canónico como función lineal de las variables predictoras.

Una vez formulado el modelo GLM, los parámetros se estiman mediante el método de máxima verosimilitud (ML), que consiste en encontrar los valores de los coeficientes que maximizan la probabilidad de observar los datos reales. Como veremos más adelante, utilizaremos modelos GLM con funciones como `glm()` [@stats] donde se emplea por defecto el método de máxima verosimilitud (ML) como técnica de estimación. Este método se implementa mediante el algoritmo Iteratively Reweighted Least Squares (IRLS), a través del cual se encuentran los parámetros óptimos ajustando iterativamente los pesos de las observaciones según su varianza esperada. Esta estrategia es apropiada para distribuciones como la binomial o la gamma y permite un ajuste robusto en situaciones donde la variable respuesta no sigue una distribución normal.

En cuanto a la validación, se analizan medidas como la devianza o el AIC para comparar modelos. También es importante evaluar la distribución de la variable respuesta y el ajuste del modelo a través de gráficos de residuos.

### Ejemplo práctico

Supongamos que queremos modelar el número de llamadas que recibe un centro de emergencias por hora (`llamadas`), en función del número de operadores de guardia (`operadores`). Este tipo de datos es típico de una distribución Poisson, por lo que analizar estos datos mediante una regresión lineal tradicional no sería apropiado.

```{r include=FALSE, warning=FALSE, message=FALSE}
set.seed(123)
n <- 100
operadores <- sample(1:5, n, replace = TRUE)
lambda <- exp(0.5 + 0.3 * operadores)
llamadas <- rpois(n, lambda)
df_poisson <- data.frame(llamadas, operadores)

modelo_poisson <- glm(llamadas ~ operadores, family = poisson, data = df_poisson)
intercepto <- coef(modelo_poisson)[1]
coef_operadores <- coef(modelo_poisson)[2]
summary(modelo_poisson)
```

En este modelo, estamos modelando el logaritmo del número esperado de llamadas como una función lineal del número de operadores. Tras ajustar el modelo a una muestra simulada de 100 observaciones, obtenemos la siguiente expresión para el número esperado de llamadas por hora: $$
\mathbb{E}[\text{llamadas}] = \exp(0.658 + 0.258 \cdot \text{operadores})
$$ La interpretación de los coeficientes es en términos del logaritmo de la tasa: por cada operador adicional, el número esperado de llamadas se multiplica por **`r round(exp(coef_operadores), 3)`**; es decir, aumenta un 29.4%.

## Modelos Lineales Generalizados Mixtos (GLMM)

Como ya hemos mencionado anteriormente, cuando trabajamos con datos longitudinales cuya variable respuesta no sigue una distribución normal, los Modelos Lineales Mixtos (LMM) dejan de ser apropiados. En estos casos, podemos extender los modelos hacia los Modelos Lineales Generalizados Mixtos (GLMM), los cuales combinan la flexibilidad de los GLM con la estructura de efectos aleatorios de los LMM. Un GLMM permite modelar variables respuesta que pertenecen a la familia exponencial (binomial, Poisson, etc.), y la correlación entre medidas repetidas para el mismo individuo mediante efectos aleatorios.

La ecuación general de un GLMM [@mcculloch2008glmms] es: $$
g\left(\mathbb{E}(y_{ij} \mid \mathbf{b}_i)\right) = \mathbf{x}_{ij}^\top \boldsymbol{\beta} + \mathbf{z}_{ij}^\top \mathbf{b}_i
$$ Donde:

-   $y_{ij}$ es la respuesta del individuo $i$ en la ocasión $j$.

-   $\mathbf{x}_{ij}$ es el vector de covariables con efectos fijos.

-   $\boldsymbol{\beta}$ es el vector de coeficientes fijos.

-   $\mathbf{z}_{ij}$ es el vector de covariables con efectos aleatorios.

-   $\mathbf{b}_i$ es el vector de efectos aleatorios del individuo $i$, que se asume que sigue una distribución normal multivariante $\mathbf{b}_i \sim \mathcal{N}(0, \mathbf{D})$.

-   $g(\cdot)$ es la función de enlace, que conecta la media condicional de la respuesta con la combinación lineal de predictores.

Dependiendo de la naturaleza de $y_{ij}$, tendremos que utilizar distintos enlaces. Para datos binarios, usaremos un enlace logit y distribución binomial; para datos de conteo, utilizaremos un enlace log y distribución de Poisson; y para tiempos o proporciones, emplearemos enlaces adaptados como log-log, logit, etc. Si la variable respuesta es binaria (por ejemplo, éxito/fracaso), se usa un modelo logístico mixto:

$$
\text{logit}(p_{ij}) = \beta_0 + \beta_1 x_{ij} + b_i
$$

Donde:

-   $\text{logit}(p) = \log\left(\frac{p}{1 - p}\right)$

-   $p_{ij} = \mathbb{P}(y_{ij} = 1 \mid b_i)$

-   $b_i \sim \mathcal{N}(0, \sigma^2_b)$ es un efecto aleatorio por sujeto.

Esto permite modelar probabilidades condicionales considerando la variabilidad entre individuos.

### Ejemplo práctico

Supongamos que queremos modelar si un estudiante aprueba un examen (`aprobado = 0/1`) en función de las horas de estudio (`horas`) y si el estudiante forma parte de un grupo diferente (`grupo`). Como los estudiantes tienen múltiples exámenes, añadimos un efecto aleatorio por estudiante.

```{r include=FALSE, warning=FALSE, message=FALSE}
library(lme4)

# Simulación de datos 
set.seed(123)
n_estudiantes <- 100
n_examenes <- 5
df_glmm <- expand.grid(
  estudiante = factor(1:n_estudiantes),
  examen = 1:n_examenes
)
df_glmm$horas <- runif(nrow(df_glmm), 0, 10)
df_glmm$grupo <- sample(c("A", "B"), nrow(df_glmm), replace = TRUE)

# Probabilidad de aprobar en función de horas y grupo, más el efecto aleatorio por estudiante
df_glmm$prob <- plogis(-1 + 0.4 * df_glmm$horas + ifelse(df_glmm$grupo == "B", 0.5, 0) + rnorm(n_estudiantes)[df_glmm$estudiante])
df_glmm$aprobado <- rbinom(nrow(df_glmm), 1, df_glmm$prob)

# Ajuste del modelo GLMM
modelo_glmm <- glmer(aprobado ~ horas + grupo + (1 | estudiante), 
                     data = df_glmm, family = binomial)

summary(modelo_glmm)
```

En este modelo GLMM (`glmer`) [@lme4] estamos modelando la probabilidad de aprobar un examen en función de dos variables explicativas: horas (número de horas de estudio) y grupo (grupo educativo (A o B)). Además, incluimos un efecto aleatorio de intercepto por estudiante, lo cual es adecuado porque cada estudiante tiene múltiples observaciones (exámenes), y esperamos que haya variabilidad entre ellos.

El intercepto aleatorio por estudiante tiene una desviación estándar de **0.948**, lo que indica que hay una variación importante en la tendencia a aprobar entre estudiantes, incluso tras haber controlado mediante las horas de estudio y el grupo; lo que justifica el uso de un GLMM en lugar de un GLM clásico. Este problema justifica cómo un GLMM puede capturar variabilidad individual (entre estudiantes) y a la vez evaluar el impacto de efectos fijos. El uso del modelo mixto es clave, ya que si ignoramos el efecto aleatorio por estudiante, estaríamos asumiendo que todos los estudiantes tienen la misma tendencia a aprobar, lo cual podemos apreciar no es el caso según la varianza estimada.

Los GLMM tienen múltiples ventajas respecto a otros modelos, ya que permiten ajustar modelos a variables respuesta no continuas, incorporan variabilidad entre individuos mediante efectos aleatorios, se adaptan bien a datos longitudinales, y permiten sacar conclusiones globales considerando la dependencia temporal. Por tanto, los GLMM son un método esencial para el análisis de datos longitudinales cuando la respuesta no es normal, ya que preservan la estructura de dependencia de los datos sin violar los supuestos del modelo.

La estimación de los parámetros en modelos GLMM es más compleja que en GLM o LMM, ya que la función de verosimilitud no tiene una forma analítica cerrada. Por ello, se utilizan técnicas de aproximación numérica como la aproximación de Laplace, que integra los efectos aleatorios y aproxima la verosimilitud, la cuadratura Gauss-Hermite adaptativa, que mejora la precisión en presencia de muchos efectos aleatorios, o Penalized Quasi-Likelihood (PQL), en versiones simplificadas. Estas aproximaciones permiten sacar conclusiones sobre los efectos fijos y estimar la variabilidad entre sujetos. Además, los GLMM pueden presentar problemas de convergencia, especialmente con estructuras complejas o tamaños muestrales pequeños. Es importante validar el modelo revisando los residuos, la bondad de ajuste (AIC, BIC), y la significancia de los efectos aleatorios, por ejemplo, mediante test de razón de verosimilitudes anidados o comparaciones de modelos.

En la práctica, utilizaremos la función `glmmTMB` [@glmmTMB2025] para ajustar modelos GLMM. Esta elección se debe a que `glmmTMB` ofrece una mayor flexibilidad que otras alternativas como `glmer`, permitiendo especificar una amplia variedad de distribuciones (incluidas binomial, Poisson, gamma o beta) y enlaces personalizados. Además, presenta una robustez computacional que resulta de gran utilidad en contextos con estructuras complejas de efectos aleatorios. Esta versatilidad lo convierte en una herramienta adecuada para los distintos tipos de variables respuesta que pueden encontrarse en el análisis de datos longitudinales.

## Validación del modelo y predicciones

Una vez ajustado un modelo mixto, debemos comprobar que el modelo se adapta adecuadamente a los datos y que cumple los supuestos teóricos necesarios antes de realizar predicciones o extraer conclusiones. En modelos LMM, esto se produce mediante un análisis de residuos (residuos vs ajustados, QQ-plot, trayectorias individuales) que realizaremos con el paquete `DHARMa`, el cual genera residuos simulados sobre los que aplicar tests de uniformidad, dispersión y detección de valores atípicos. En modelos GLMM, aunque el principio es parecido, la validación debe adaptarse al tipo de variable respuesta, utilizando medidas como el AIC para comparar modelos y comprobaciones como la convergencia del ajuste o la significancia de los efectos aleatorios. Validar el modelo permiten concluir si el modelo es adecuado para los datos; garantizando que sus predicciones sean fiables y que las conclusiones son robustas.

Una vez validado el modelo, este puede ser utilizado para hacer predicciones. En el caso de un modelo mixto bien ajustado, las predicciones permiten estimar el valor esperado de una variable en un momento determinado considerando tanto la tendencia general como las diferencias individuales. A través del uso de modelos validados, los LMM y GLMM se postulan como métodos muy eficaces a la hora de generar estimaciones, hacer predicciones futuras o evaluar situaciones hipotéticas.

---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Modelos mixtos

```{r message=FALSE, warning=FALSE, include=FALSE}
library(ggplot2)
knitr::opts_chunk$set(
  fig.width = 6, 
  fig.height = 4, 
  dev = "png",
  fig.align = "center"
)
theme_set(theme_minimal(base_size = 12))
```


En este capítulo, exploraremos los **Modelos Lineales Mixtos (LMM)** y los **Modelos Lineales Generalizados (GLM)**, dos enfoques estadísticos fundamentales para el análisis de datos longitudinales. Veremos cómo los LMM permiten modelar la variabilidad entre individuos mediante la inclusión de efectos aleatorios y fijos, lo que facilita el estudio de la correlación entre observaciones repetidas. Luego, introduciremos los GLM, que extienden la regresión lineal para manejar variables respuesta que no siguen una distribución normal, utilizando funciones de enlace y la familia exponencial. A lo largo del capítulo, revisaremos sus formulaciones matemáticas, sus hipótesis clave y cómo validarlas en la práctica.

Para ilustrar estos modelos, comenzaremos con un ejemplo aplicado al conjunto de datos Orthodont del paquete nlme, donde analizaremos la evolución de la distancia entre los dientes (distance) en función de la edad (age) en diferentes sujetos. Compararemos tres enfoques distintos:

-   Modelo con sólo **efectos fijos**: Se asume que todos los sujetos siguen la misma relación.

-   Modelo con sólo **efectos aleatorios**: Se permite que cada sujeto tenga su propio valor inicial (intercepto), pero no afecta la pendiente.

-   **Modelo mixto**: Se permite que tanto el intercepto como la pendiente varíen entre sujetos.

## Comparación de modelos con efectos fijos, aleatorios y mixtos

La base de datos Orthodont proviene del paquete nlme en R y contiene información sobre el crecimiento dental en niños. Sus variables principales son:

-   distance: distancia entre los dientes (variable respuesta).

-   age: edad del niño (variable predictora principal).

-   Subject: identificador del niño (variable de agrupación para efectos aleatorios).

A continuación, ajustaremos y visualizaremos los distintos modelos.

### Modelo con efectos fijos

El primer modelo que consideramos es una regresión lineal simple, en la que asumimos que la distancia interdental (distance) varía en función de la edad (age), pero asumimos que todas las observaciones son independientes e ignoramos la estructura jerárquica del estudio (mediciones repetidas por individuo). La ecuación del modelo es: $$
distance_i = \beta_0 + \beta_1 age_i + \epsilon_i$$
Aquí, $distance_i$ es la distancia interdental de la observación $i$, $\beta_0$ es la intersección común a todos los sujetos, $\beta_1$ es la pendiente (cómo cambia la distancia con la edad), y $\epsilon_i$ es el error aleatorio.

```{r include=FALSE, warning=FALSE, message=FALSE}
# Cargar paquetes
library(nlme)
library(lme4)

# Cargar datos longitudinales
data(Orthodont, package = "nlme")

# Ajustar modelo de regresión lineal simple (solo efectos fijos)
modelo_fijo <- lm(distance ~ age, data = Orthodont)

# Resumen del modelo
r2 <- summary(modelo_fijo)$r.squared
```

```{r}
#| label: fig-reglinsimp
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Modelo con efectos fijos: Regresión Lineal Simple"
library(ggplot2)
ggplot(Orthodont, aes(x = age, y = distance)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Modelo con efectos fijos: Regresión Lineal Simple",
       x = "Edad", y = "Distancia interdental")

```

Este modelo de la @fig-reglinsimp considera únicamente la edad (age) como predictor de la distancia (distance) y no tiene en cuenta que los datos son mediciones repetidas de los mismos individuos, lo que puede llevar a errores de estimación debido a la correlación entre observaciones de un mismo sujeto. Como podemos comprobar a través de este ejemplo, si se ignora la estructura jerárquica, podríamos obtener estimaciones erróneas de la variabilidad en la población; obteniendo un coficiente de determinación R² bajísimo (**`r round(r2, 3)`**).

### Modelo con efectos aleatorios

Ahora ajustamos un modelo con efectos aleatorios, en el que permitimos que cada niño tenga su propio intercepto aleatorio ($u_i$), capturando la variabilidad entre individuos. La ecuación del modelo es: $$
distance_{ij} = \beta_0 + u_i +\beta_1 age_{ij} + \epsilon_{ij}$$
Ahora $distance_{ij}$ es la distancia para el individuo $i$ en la observación $j$. $u_i$ representa el efecto aleatorio de cada sujeto (intersección específica), $\beta_1$ es la pendiente común, y $\epsilon_{ij}$ es el error.

```{r include=FALSE, warning=FALSE, message=FALSE}
# Cargar paquetes
library(nlme)
library(lme4)

# Cargar datos longitudinales
data(Orthodont, package = "nlme")

modelo_aleatorio <- lmer(distance ~ age + (1 | Subject), data = Orthodont)
summary(modelo_aleatorio)


# Resumen del modelo
r2 <- summary(modelo_aleatorio)$r.squared
# Extraer la varianza del intercepto por sujeto
var_intercept <- as.numeric(VarCorr(modelo_aleatorio)$Subject[1])
# Extraer la varianza residual
var_residual <- attr(VarCorr(modelo_aleatorio), "sc")^2

```

```{r}
#| label: fig-efectos-aleatorios
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Modelo con efectos aleatorios a través de intercepto aleatorio"
library(ggplot2)
# Gráfico mejorado para modelo con efectos aleatorios
ggplot(Orthodont, aes(x = age, y = distance, group = Subject, color = Subject)) +
  geom_line(linetype = "dashed") +  # Línea discontinua para observaciones
  geom_abline(intercept = fixef(modelo_aleatorio)[1] + ranef(modelo_aleatorio)$Subject[,1],
              slope = fixef(modelo_aleatorio)[2],
              color = "black", alpha = 0.2) +  # Una línea por sujeto
  labs(title = "Modelo con efectos aleatorios (intercepto)", x = "Edad", y = "Distancia interdental") +
  theme_minimal()


```

Como podemos apreciar en la @fig-efectos-aleatorios, ahora tenemos un término indica que cada individuo (Subject) tiene su propia intersección aleatoria; permitiendo que la relación entre la distancia y la edad varíe entre individuos en lugar de asumir una única intersección fija para todos. Esto significa que algunos sujetos pueden tener valores iniciales más altos o más bajos de distance sin que eso afecte la tendencia general de la población. La diferencia crucial de los efectos aleatorios la podemos apreciar en la variabilidad del modelo, ya que tenemos una varianza del intercepto por sujeto de **`r round(var_intercept, 3)`** y una varianza residual de **`r round(var_residual, 3)`**, lo que significa que cada sujeto tiene un punto de partida diferente en distance, pero que todavía hay una parte de la variabilidad del modelo que no se explica por los efectos fijos ni por las diferencias entre sujetos.

Este modelo permite que cada niño tenga su propio intercepto aleatorio, modelando mejor la variabilidad individual.

### Modelo mixto

Finalmente, ajustamos un **Modelo Lineal Mixto (LMM)** en el que consideramos tanto efectos fijos como aleatorios. Permitimos que cada niño tenga su propio intercepto ($u_i$) y pendiente ($v_i$) aleatorios, permitiendo que la relación entre edad y distancia interdental varíe entre individuos. La ecuación del modelo es: $$
distance_{ij} = \beta_0 + u_i + (\beta_1 + v_i) age_{ij} + \epsilon_{ij}$$
Aquí $u_i$ es la intersección específica de cada sujeto, y $v_i$ permite que la pendiente también varíe por individuo.

```{r include=FALSE, warning=FALSE, message=FALSE}
# Cargar paquetes
library(nlme)
library(lme4)

# Cargar datos longitudinales
data(Orthodont, package = "nlme")

modelo_mixto <- lmer(distance ~ age + (1 + age | Subject), data = Orthodont)
summary(modelo_mixto)


# Resumen del modelo
r2 <- summary(modelo_mixto)$r.squared
# Extraer la varianza del intercepto por sujeto
var_intercept_completo <- as.numeric(VarCorr(modelo_mixto)$Subject[1])
var_pendiente_completo <- as.numeric(VarCorr(modelo_mixto)$Subject["age", "age"])
# Extraer la varianza residual
var_residual_completo <- attr(VarCorr(modelo_mixto), "sc")^2
```

```{r}
#| label: fig-modelo-lmm
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Modelo Lineal Mixto (LMM con efectos fijos y aleatorios)"
# Obtener coeficientes por individuo
coefs <- coef(modelo_mixto)$Subject

# Gráfico mejorado para modelo mixto
ggplot(Orthodont, aes(x = age, y = distance, group = Subject, color = Subject)) +
  geom_line(linetype = "dashed") +  # Línea discontinua para observaciones
  geom_abline(data = coefs, aes(intercept = `(Intercept)`, slope = age), 
              color = "black", size = 0.6, alpha = 0.4) +
  labs(title = "Modelo mixto (intercepto y pendiente aleatoria)", 
       x = "Edad", y = "Distancia interdental") +
  theme_minimal()


```

Ahora no solo permitimos una intersección aleatoria, sino que también permitimos que la pendiente (efecto de la edad) varíe entre sujetos; es decir, en este modelo cada sujeto puede tener una tasa de crecimiento diferente en la distancia dental a lo largo del tiempo. Observando el modelo de la @fig-modelo-lmm, vemos como ahora hemos reducido la varianza residual a **`r round(var_residual_completo, 3)`**, y ahora contamos con una varianza del intercepto por sujeto de **`r round(var_intercept_completo, 3)`** y una variación de la pendiente entre sujetos de **`r round(var_pendiente_completo, 3)`**; obteniendo una mejora significativa. Este tipo de modelos es más realista cuando hay variabilidad individual en la evolución de la variable respuesta.

Este modelo es más flexible, ya que permite que tanto la intersección como la pendiente de la relación entre edad y distancia varíen entre individuos. Este último modelo generaliza la idea que vimos en el capítulo anterior, donde ajustábamos una regresión por individuo (Figura 4). En aquel caso, teníamos una pendiente e intercepto diferentes por persona, pero ajustados de forma separada. Los modelos mixtos permiten hacer esto mismo, pero de forma conjunta y eficiente, combinando la información de todos los individuos para obtener estimaciones más robustas, sin necesidad de ajustar un modelo por separado para cada uno.

Si comparamos los 3 modelos, podemos observar que el modelo con solo **efectos fijos** asume una única relación entre edad y distancia interdental, ignorando la variabilidad entre individuos. El modelo con solo **efectos aleatorios** permite que cada sujeto tenga su propio intercepto, pero mantiene una pendiente común para todos. El **modelo mixto (LMM)** es el más completo, permitiendo que tanto la intersección como la pendiente varíen entre individuos. Esto demuestra la importancia de los Modelos Lineales Mixtos en el análisis de datos longitudinales, ya que incorporan tanto la variabilidad individual como la estructura jerárquica de los datos.

## Modelos Lineales Mixtos (LMM)

Son métodos y modelos estadísticos que sirven para analizar datos longitudinales cuando la variable respuesta sigue una distribución normal. Uno de sus aspectos más característicos lo indica Francisco Hernández-Barrera en su libro *Modelos mixtos con R* [@modelos_mixtos], ya que se asume que existe una relación entre el vector de observaciones y las covariables. Se considera la técnica más eficaz cuando se trabaja con distribuciones normales en este campo ya que permite introducir efectos aleatorios y concretar la estructura de las correlaciones de los residuos del mismo sujeto; además de que puede emplearse con datos faltantes. Estos modelos nos permiten modelar la correlación entre observaciones dentro de una misma unidad e incluir covariables tanto a nivel individual como grupal. Los LMM permiten realizar una estimación precisa de la incertidumbre, respetando la dependencia entre observaciones. Por otro lado, su capacidad de generalización a estructuras de datos complejas es otro de los motivos por los cuales se recomienda su uso con datos longitudinales. Otra de sus ventajas es su flexibilidad para incluir efectos específicos por individuo o grupo; algo que veremos más adelante.

La ecuación para este tipo de modelos, en los que $y_{ij}$ representa el momento $j$-ésimo del individuo $i$: $$
y_{ij} = \beta_{0i} + \sum_{k=1}^{K} \beta_{ki}x_{ijk} + e_{ij} 
$$

-   $x_{ijk}$ es el valor de la k-ésima variable independiente por parte del individuo $i$ en la observación $j$.

-   $\beta_{0i}$ sigue $N(\beta_{0}, \sigma^2_{\beta_{0}})$; es el intercepto del modelo, que suele tener cierta varianza centrada en $\mu$ porque se supone aleatoria.

-   $\beta_{ki}$ sigue $N(\beta_{k}, \sigma^2_{\beta_{k}})$; son las pendientes o coeficientes de las variables independientes del modelo, que suelen ser aleatorias.

Los **efectos aleatorios** se representan mediante el vector formado por la constante y los coeficientes aleatorios del modelo. Nos permiten capturar la variabilidad entre individuos, y se escriben de esta forma: $$
\vec{\beta}_i = (\beta_{0i}, \beta_{1i}, \ldots, \beta_{Ki})^t \sim N(\vec{\beta}, \Omega)
$$ Cabe destacar que los errores de un individuo, al no tener todos el mismo número de observaciones, son **independientes** de los efectos aleatorios.

Para ajustar un modelo lineal mixto, se tienen que disponer los datos de forma vertical. Una de las ventajas del LMM es su flexibilidad ya que no sólo permite especificar efectos aleatorios para evaluar la **variabilidad** de algunas variables entre los individuos, sino que también permite evaluar la **correlación** entre distintos datos longitudinales del mismo individuo. La constante y los coeficientes aleatorios tienen **homocedasticidad**, ya que la esperanza y la matriz de covarianzas es la misma para todos los individuos. Una de las características de los LMM es que introducen el concepto de **efectos fijos**, los cuales son la esperanza de los efectos aleatorios. Según Julian Faraway en *Extending the Linear Model with R: Generalized Linear, Mixed Effects and Nonparametric Regression Models* [@faraway], un efecto fijo es una constante desconocida que intentaremos estimar a partir de los datos, mientras que los efectos aleatorios son variables aleatorias. De hecho, cuando un coeficiente no es aleatorio, se puede asumir que sigue una distribución normal con varianza cero; denominándolo fijo. En estos modelos, el número de efectos aleatorios es limitado, ya que no pueden superar en ningún caso el número de medidas que tenemos por individuo. Estos efectos inducen una **correlación** entre datos para el mismo individuo, pero dependiendo de su estructura la correlación sólo se puede obtener a partir de la correlación entre residuos; ya que si consideramos los coeficientes de variables **cambiantes** en el tiempo como aleatorios la correlación es distinta según los tiempos de las medidas, mientras que si consideramos los coeficientes de variables **constantes** en el tiempo inducimos **heterocedasticidad** entre individuos.

Una vez formulado el modelo, necesitamos estimar los parámetros, contando con los dos principales métodos de estimación en modelos mixtos. El primero es el método de Máxima verosimilitud (ML), que utiliza la función de verosimilitud completa del modelo. Estima tanto los efectos fijos como las varianzas de los efectos aleatorios. Es útil para comparar modelos con diferentes efectos fijos. El otro método es el de máxima verosimilitud restringida (REML), que estima solo las varianzas de los efectos aleatorios, ajustando los grados de libertad para evitar el sesgo en la estimación de la varianza. Es el método preferido para comparar modelos con la misma estructura de efectos fijos pero distinta estructura de efectos aleatorios.

A la hora de trabajar con Modelos Lineales Mixtos, se puede trabajar de diferentes formas. Podemos establecer un modelo con la constante aleatoria y varios coeficientes fijos en el tiempo, en cuyo caso, si asumimos que los errores son independientes, tendríamos una correlación **constante** entre las variables del mismo individuo que no depende de la distancia entre las medidas; lo que se denomina como coeficiente de correlación intraclase (ICC). Otra forma de definir estos modelos podría ser con la constante y los coeficientes aleatorios, donde, asumiendo independencia entre residuos, la correlación entre observaciones pasa a depender tanto del tiempo como de la distancia entre ellas. Sin embargo, pese a ser las dos buenas opciones, es preferible trabajar de otra forma para LMM.

Para empezar, no asumiremos independencia de los residuos; sino que trabajaremos con un modelo más general en el que contemos con el mayor número posible de efectos aleatorios correlacionados y fijos. A continuación, procederemos a simplificar el modelo a través de la significación de **efectos aleatorios**: $$
\begin{cases} 
H_0 : \sigma^2_{\beta_0} = 0 \\ 
H_1 : \sigma^2_{\beta_0} > 0 
\end{cases}
$$ Para comprobar que hay más de un efecto aleatorio significativo, se utilizan diferentes técnicas estadísticas para contrastar que se permite asumir que podemos rechazar la hipótesis nula: los efectos aleatorios tienen varianza igual a cero. En caso afirmativo, tenemos que contrastar que si su correlación es distinta de 0; para lo que tendremos que elegir la matriz de covarianzas de los efectos aleatorios: $$
\begin{cases}
H_0 : \Omega = 
\begin{pmatrix}
\sigma^2_{\beta_0} & 0 \\
0 & \sigma^2_{\beta_1}
\end{pmatrix} \\
H_1 : \Omega = 
\begin{pmatrix}
\sigma^2_{\beta_0} & \sigma_{\beta_0 \beta_1} \\
\sigma_{\beta_0 \beta_1} & \sigma^2_{\beta_1}
\end{pmatrix}
\end{cases}
$$ En este caso, utilizaremos un test de razón de verosimilitudes para escoger la estructura de covarianzas de los efectos aleatorios y de sus errores. Para ello, hay que tener en cuenta que los modelos estén **anidados**, es decir, que la matriz de covarianzas de los residuos de un modelo se expresen como un caso particular de la matriz de covarianzas de los residuos del otro modelo.

Una vez hemos terminado con los efectos aleatorios, procedemos a determinar la significación de los efectos fijos a través de dos métodos. Si queremos testear un sólo parámetro, utilizaremos el test de Wald en el que: $$
\begin{cases}
H_0 : \beta_1 = 0 \\
H_1 : \beta_1 \neq 0
\end{cases}
$$ En caso de querer testear más de un parámetro, utilizaremos un test de razón de verosimilitudes: $$
\begin{cases}
H_0 : \beta_1 = \beta_2 = 0 \\
H_1 : \text{alguno diferente de 0}
\end{cases}
$$ Una vez hemos definido ya nuestro modelo, tenemos que realizar su validación a través de comprobar que se cumplen las asunciones sobre los residuos; al igual que hacíamos con Regresión Lineal Simple. Para poder asumir que el modelo es correcto, en el gráfico **residuos estandarizados vs valores predichos**, debería de aparecer una especie de nube de puntos en los que no haya ningún patrón ni ninguna tendencia aparente; mientras que en el **QQ-plot**, si los residuos se encuentran alrededor de la diagonal sin seguir tampoco ningún patrón, podremos asumir que los residuos tienen normalidad. Para validar lod efectos aleatorios, podemos utilizar **Empirical Bayes Estimates** en lugar de asumir su normalidad.

## Modelos Lineales Generalizados (GLM)

Los Modelos Lineales Generalizados son una generalización de los modelos lineales para una variable respuesta perteneciente a la familia exponencial, en la que tenemos una función de enlace que describe como la media de la variable respuesta y la combinación lineal de variables explicativas están relacionadas. Según Paul Roback y Julie Regler en el libro *Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R*[@roback_legler], los GLM son una clase de modelos más amplia que tienen formas parecidas para sus varianzas, verosimilitudes y MLEs; generalizando la regresión lineal múltiple.

La **familia exponencial** suele tener esta forma: $$
f(y \mid \theta, \phi) = \exp \left[ \frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi) \right]
$$ En esta ecuación, $\theta$ es el **parámetro canónico** y representa la posición (location); mientras que $\phi$ es el **parámetro de dispersión** y representa la escala (scale). De la misma forma, $a$, $b$ y $c$ representan diferentes miembros de la familia exponencial. En función del parámetro de dispersión, podemos distinguir entre familias exponenciales de **un** parámetro, y familias exponenciales de **dos** parámetros.

Para determinar si un modelo está basado en un único parámetro $\theta$, tenemos que poder escribir su función de probabilidad de la siguiente forma: $$
f(y; \theta) = e^{[a(y)b(\theta) + c(\theta) + d(y)]}
$$ Si el conjunto de posibles valores de entrada no depende de $\theta$, la familia exponencial será de un parámetro. Como familias exponenciales de un parámetro, tenemos las distribuciones de Poisson y la Binomial. Vamos a demostrar que la distribución de Poisson es, en efecto, una familia exponencial de un parámetro.

Para ello, aplicando propiedades logarítmicas, podemos definir la distribución de Poisson como: $$
P(Y = y) = e^{-\lambda} e^{y \log \lambda} e^{-\log(y!)} \\
= e^{y \log \lambda - \lambda - \log(y!)}
$$ Si comparamos esta función de masa de probabilidad con la fnción de probabilidad general para familias con un único parámetro, podemos ver que: $$
\begin{aligned}
a(y) &= y \\
b(\theta) &= \log(\lambda) \\
c(\theta) &= -\lambda \\
d(y) &= -\log(y!)
\end{aligned}
$$ La función $b(\theta)$ es lo que denominamos **enlace canónico**, una función que nos permite modelar como una función lineal de variables explicativas.

Como familias exponenciales de dos parámetros, tenemos la distribución Gamma y la Normal. De forma parecida a la anterior, podemos demostrar que la distribución Normal es una familia exponencial de dos parámetros.

Podemos definir la función de densidad de una distribución Normal como: $$
f(y | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
$$ Si separamos términos y los escribimos como términos logarítmicos, tenemos que: $$
f(y | \mu, \sigma^2) = \exp \left( y \cdot \frac{\mu}{\sigma^2} - \frac{y^2}{2\sigma^2} + \left( -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2) \right) \right)
$$ Si comparamos esta función de densidad con la forma general de la familia exponencial, podemos ver que: $$
\begin{aligned}
a(y) = y\\
b(\mu, \sigma^2) = \frac{\mu}{\sigma^2}\\
c(\mu, \sigma^2) = -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\\
d(y, \sigma^2) = -\frac{y^2}{2\sigma^2}\\
\end{aligned}
$$ Por lo tanto, demostramos que la distribución normal también pertenece a la familia exponencial, pero con una peculiaridad respecto a la distribución de Poisson: es una familia exponencial de dos parámetros, la media $\mu$ y la varianza $\sigma^2$. En este caso, el término $b(\mu, \sigma^2)$ es el **enlace canónico** que conecta las variables explicativas con el modelo.

En concreto, para los casos en los que la respuesta no es normal, la ecuación del modelo es la siguiente: $$
g\left(E(y_{ij} \mid x_{ijk}, \beta_{0i}, \dots, \beta_{Ki})\right) = \beta_{0i} + \sum_{k=1}^{K} \beta_{ki}x_{ijk}
$$ Donde g es la función enlace y, pese a que puede parecerse mucho a la función para modelos LMM, tienen algunas diferencias, como que en el primer miembro tenemos el enlace del valor esperado en vez de la variable respuesta, y en el segundo miembro no se cuenta con los errores; por lo que no existe una matriz de correlaciones de los residuos. De esta forma, ya hemos **generalizado** nuestro modelo para manejar variables respuesta que no siguen una distribución normal. A través de esta generalización, somos capaces de escribir la función de masa o densidad de probabilidad de distintas distribuciones para poder modelar el enlace canónico como función lineal de las variables predictoras.

Una vez formulado el modelo GLM, los parámetros se estiman mediante el método de Máxima Verosimilitud (MLE). Para obtener los estimadores, se utiliza un procedimiento iterativo denominado Iteratively Reweighted Least Squares (IRLS), que ajusta los pesos de las observaciones en función de la varianza esperada.

En cuanto a la validación, se analizan medidas como la devianza (comparable a la suma de cuadrados en modelos lineales), el AIC para comparar modelos, y los residuos devianza y Pearson, que permiten detectar valores atípicos o mal ajuste. También es importante evaluar la distribución de la variable respuesta y el ajuste del modelo a través de gráficos de residuos.

### Ejemplo práctico 

Supongamos que queremos modelar el número de llamadas que recibe un centro de emergencias por hora (llamadas), en función del número de operadores de guardia (operadores). Este tipo de datos es típico de una distribución Poisson.

```{r echo=FALSE, warning=FALSE, message=FALSE}
set.seed(123)
n <- 100
operadores <- sample(1:5, n, replace = TRUE)
lambda <- exp(0.5 + 0.3 * operadores)
llamadas <- rpois(n, lambda)
df_poisson <- data.frame(llamadas, operadores)

modelo_poisson <- glm(llamadas ~ operadores, family = poisson, data = df_poisson)
summary(modelo_poisson)
```

En este modelo, estamos modelando el logaritmo del número esperado de llamadas como una función lineal del número de operadores. La interpretación de los coeficientes es en términos del logaritmo de la tasa: por cada operador adicional, la tasa de llamadas aumenta multiplicativamente por exp(β).

## Modelos Lineales Generalizados Mixtos (GLMM)

Cuando trabajamos con datos longitudinales cuya variable respuesta no sigue una distribución normal, los Modelos Lineales Mixtos (LMM) dejan de ser apropiados. En estos casos, extendemos los modelos hacia los Modelos Lineales Generalizados Mixtos (GLMM), los cuales combinan la flexibilidad de los GLM con la estructura de efectos aleatorios de los LMM. Un GLMM permite modelar: variables respuesta que pertenecen a la familia exponencial (binomial, Poisson, Gamma...), y la correlación entre observaciones repetidas para el mismo individuo, mediante efectos aleatorios.

La ecuación general de un GLMM es:
$$
g\left(\mathbb{E}(y_{ij} \mid b_i)\right) = \beta_0 + \sum_{k=1}^{K} \beta_k x_{ijk} + Z_{ij}b_i
$$
Donde:

- $y_{ij}$ es la respuesta del individuo $i$ en la ocasión $j$.

- $x_{ijk}$ es el valor de la variable explicativa $k$ para ese individuo en ese momento.

- $\beta_{0i}$ es el intercepto aleatorio.

- $\beta_{ki}$ puede incluir efectos fijos o aleatorios.

- $g(\cdot)$ es la función de enlace, que conecta el valor esperado con la combinación lineal de predictores.

Dependiendo de la naturaleza de $y_{ij}$, usaremos distintos enlaces. Para datos binarios: enlace logit y distribución binomial; para datos de conteo: enlace log y distribución de Poisson; y para tiempos o proporciones: enlaces adaptados como log-log, logit, etc. Si la variable respuesta es binaria (por ejemplo, éxito/fracaso), se usa un **modelo logístico mixto**:

$$
\text{logit}(p_{ij}) = \beta_0 + \beta_1 x_{ij} + b_i
$$

Donde:

- $\text{logit}(p) = \log\left(\frac{p}{1 - p}\right)$
- $p_{ij} = \mathbb{P}(y_{ij} = 1 \mid b_i)$
- $b_i \sim \mathcal{N}(0, \sigma^2_b)$ es un efecto aleatorio por sujeto.

Esto permite modelar probabilidades condicionales considerando la variabilidad entre individuos.

### Ejemplo práctico

Supongamos que queremos modelar si un estudiante aprueba un examen (`aprobado = 0/1`) en función de las horas de estudio (`horas`) y si el estudiante forma parte de un grupo diferente (`grupo`). Como los estudiantes tienen múltiples exámenes, añadimos un efecto aleatorio por `estudiante`.

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(lme4)

# Simulación de datos ejemplo
set.seed(123)
n_estudiantes <- 100
n_examenes <- 5
df_glmm <- expand.grid(
  estudiante = factor(1:n_estudiantes),
  examen = 1:n_examenes
)
df_glmm$horas <- runif(nrow(df_glmm), 0, 10)
df_glmm$grupo <- sample(c("A", "B"), nrow(df_glmm), replace = TRUE)

# Probabilidad de aprobar en función de horas y grupo, más efecto aleatorio por estudiante
df_glmm$prob <- plogis(-1 + 0.4 * df_glmm$horas + ifelse(df_glmm$grupo == "B", 0.5, 0) + rnorm(n_estudiantes)[df_glmm$estudiante])
df_glmm$aprobado <- rbinom(nrow(df_glmm), 1, df_glmm$prob)

# Ajuste del modelo GLMM
modelo_glmm <- glmer(aprobado ~ horas + grupo + (1 | estudiante), 
                     data = df_glmm, family = binomial)

summary(modelo_glmm)
```

En este modelo GLMM (glmer) estamos modelando la probabilidad de aprobar un examen en función de dos variables explicativas: horas (número de horas de estudio) y grupo (grupo educativo (A o B), con A como categoría de referencia). Además, incluimos un efecto aleatorio de intercepto por estudiante, lo cual es adecuado porque cada estudiante tiene múltiples observaciones (exámenes), y esperamos que haya variabilidad entre ellos.

El intercepto aleatorio por estudiante tiene una desviación estándar de 0.9473, lo que indica que hay una variación importante en la propensión a aprobar entre estudiantes, incluso tras controlar por las horas de estudio y el grupo; lo que justifica el uso de un GLMM en lugar de un GLM clásico. Este modelo demuestra cómo un GLMM puede capturar variabilidad individual (entre estudiantes) y a la vez evaluar el efecto de factores fijos. El uso del modelo mixto es crucial: si ignoramos el efecto aleatorio por estudiante, estaríamos asumiendo que todos los estudiantes tienen la misma propensión a aprobar, lo cual claramente no es el caso según la varianza estimada.

Los GLMM tienen múltiples ventajas respecto a otros modelos, ya que permiten ajustar modelos a variables respuesta no continuas, incorporan variabilidad entre individuos mediante efectos aleatorios, se adaptan bien a datos longitudinales y jerárquicos, y permiten hacer inferencia poblacional y considerar la dependencia temporal. Por tanto, los GLMM constituyen una herramienta esencial para el análisis de datos longitudinales cuando la respuesta no es normal, ya que preservan la estructura de dependencia de los datos sin violar los supuestos del modelo.

La estimación de los parámetros en modelos GLMM es más compleja que en GLM o LMM, debido a que la función de verosimilitud no tiene una forma analítica cerrada. Por ello, se utilizan técnicas de aproximación numérica como: aproximación de Laplace, que integra los efectos aleatorios y aproxima la verosimilitud, cuadratura Gauss-Hermite adaptativa, que mejora la precisión en presencia de muchos efectos aleatorios, o Penalized Quasi-Likelihood (PQL), en versiones simplificadas. Estas aproximaciones permiten realizar inferencia sobre los efectos fijos y estimar la variabilidad entre sujetos.

Además, los GLMM pueden presentar problemas de convergencia, especialmente con estructuras complejas o tamaños muestrales pequeños. Es importante validar el modelo revisando los residuos, la bondad de ajuste (AIC, BIC), y la significación de los efectos aleatorios, por ejemplo, mediante test de razón de verosimilitudes anidados o comparaciones de modelos.

## Validación del modelo y predicciones

Una vez ajustado un modelo, es fundamental evaluar su validez antes de utilizarlo para hacer predicciones o sacar conclusiones. La validación del modelo garantiza que los supuestos se cumplen razonablemente bien y que el modelo generaliza adecuadamente a nuevos datos. Para los LMM, la validación se basa principalmente en el análisis de residuos: residuos vs valores ajustados, donde debería observarse una nube de puntos sin patrón, lo que indica homocedasticidad y que la media de los errores es cero; QQ-plot de los residuos, que permite verificar si los residuos siguen una distribución normal; y los gráficos de predicciones por individuo que ayudan a ver si el modelo capta bien las trayectorias individuales. Además, se puede usar el paquete DHARMa para generar residuos simulados que permiten aplicar tests como testUniformity, que evalúa si los residuos simulados son uniformes; testDispersion, que evalúa si hay sobre o subdispersión; y testOutliers, que detecta observaciones atípicas.

En modelos GLMM, el proceso es similar pero adaptado al tipo de variable respuesta, ya que se analizan residuos devianza o residuos de Pearson, se verifica la bondad de ajuste mediante medidas como el AIC (cuanto más bajo, mejor), y se comprueba la convergencia del modelo y la significación de los efectos aleatorios mediante test de razón de verosimilitudes. Estas comprobaciones permiten concluir si el modelo es adecuado para los datos.

Una vez validado el modelo, se puede utilizar para hacer predicciones. En el caso de un modelo mixto bien ajustado, estas predicciones permiten estimar, por ejemplo, el valor esperado de una variable en un nuevo tiempo o bajo determinadas condiciones, considerando tanto la tendencia general como las diferencias individuales. Este uso de los modelos validados convierte a los LMM y GLMM en herramientas muy potentes para generar estimaciones robustas, hacer proyecciones futuras o evaluar distintos escenarios hipotéticos.
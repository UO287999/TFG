---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Capítulo 3: Modelos mixtos

## Modelos Lineales Mixtos (LLM)

Son métodos y modelos estadísticos que sirven para analizar datos longitudinales cuando la variable respuesta sigue una distribución normal. Se considera la técnica más eficaz cuando se trabaja con distribuciones normales en este campo ya que permite introducir efectos aleatorios y concretar la estructura de las correlaciones de los residuos del mismo sujeto; además de que puede emplearse con datos faltantes. Estos modelos nos permiten modelar la correlación entre observaciones dentro de una misma unidad e incluir covariables tanto a nivel individual como grupal. Permiten realizar una estimación precisa de la incertidumbre, respetando la dependencia entre observaciones, y la capacidad de generalización a estructuras de datos complejas son otro de los motivos por los cuales se recomienda su uso con datos longitudinales. Otra de sus ventajas es su flexibilidad para incluir efectos específicos por individuo o grupo; algo que veremos más adelante.

La ecuación para este tipo de modelos, en los que $i$ es el individuo y $j$ la observación es la siguiente:
$$
y_{ij} = \beta_{0i} + \sum_{k=1}^{K} \beta_{ki}x_{ijk} + e_{ij} 
$$ 

-   $x_{ijk}$ es el valor de la k-ésima variable independiente por parte del individuo $i$ en la observación $j$.

-   $\beta_{0i}$ sigue $N(\beta_{0}, \sigma^2_{\beta_{0}})$; es la constante del modelo, que suele tener cierta varianza centrada en $\mu$ porque se supone aleatoria.

-   $\beta_{ki}$ sigue $N(\beta_{k}, \sigma^2_{\beta_{k}})$; son las pendientes o coeficientes de las variables independientes del modelo, que suelen ser aleatorias.


Los **efectos aleatorios** es el vector formado por la constante y los coeficientes aleatorios del modelo. Nos permiten capturar la variabilidad entre individuos, y se escriben de esta forma:
$$
\vec{\beta}_i = (\beta_{0i}, \beta_{1i}, \ldots, \beta_{Ki})^t \sim N(\vec{\beta}, \Omega)
$$
Cabe destacar que los errores de un individuo, al no tener todos el mismo número de observaciones, son **independientes** de los efectos aleatorios.

Para ajustar un modelo lineal mixto, se tienen que disponer los datos de forma vertical. Una de las ventajas del LLM es su flexibilidad ya que no sólo permite especificar efectos aleatorios para evaluar la **variabilidad** de algunas variables entre los individuos, sino que también permite evaluar la **correlación** entre distintos datos longitudinales del mismo individuo. La constante y los coeficientes aleatorios tienen **homocedasticidad**, ya que la esperanza y la matriz de covarianzas es la misma para todos los individuos. Una de las características de los LLM es que introducen el concepto de **efectos fijos**, los cuales son la esperanza de los efectos aleatorios. De hecho, cuando un coeficiente no es aleatorio, se puede asumir que sigue una distribución normal con varianza cero; denominándolo fijo. En estos modelos, el número de efectos aleatorios es limitado, ya que no pueden superar en ningún caso el número de medidas que tenemos por individuo. Estos efectos inducen una **correlación** entre datos para el mismo individuo, pero dependiendo de su estructura la correlación sólo se puede obtener a partir de la correlación entre residuos; ya que si consideramos los coeficientes de variables **cambiantes** en el tiempo como aleatorios la correlación es distinta según los tiempos de las medidas, mientras que si consideramos los coeficientes de variables **constantes** en el tiempo inducimos **heterocedasticidad** entre individuos.

A la hora de trabajar con Modelos Lineales Mixtos, se puede trabajar de diferentes formas. Podemos establecer un modelo con la constante aleatoria y varios coeficientes fijos en el tiempo, en cuyo caso, si asumimos que los errores son independientes, tendríamos una correlación **constante** entre las variables del mismo individuo que no depende de la distancia entre las medidas; lo que se denomina como coeficiente de correlación intraclase (ICC). Otra forma de definir estos modelos podría ser con la constante y los coeficientes aleatorios, donde, asumiendo independencia entre residuos, la correlación entre observaciones pasa a depender tanto del tiempo como de la distancia entre ellas. Sin embargo, pese a ser las dos buenas opciones, es preferible trabajar de otra forma para LLM.

Para empezar, no asumiremos independencia de los residuos; sino que trabajaremos con un modelo más general en el que contemos con el mayor número posible de efectos aleatorios correlacionados y fijos. A continuación, procederemos a simplificar el modelo a través de la significación de **efectos aleatorios**:
$$
\begin{cases} 
H_0 : \sigma^2_{\beta_0} = 0 \\ 
H_1 : \sigma^2_{\beta_0} > 0 
\end{cases}
$$
Para comprobar que hay más de un efecto aleatorio significativo, se utilizan diferentes técnicas estadísticas para contrastar que se permite asumir que podemos rechazar la hipótesis nula: los efectos aleatorios tienen varianza igual a cero. En caso afirmativo, tenemos que contrastar que si su correlación es distinta de 0; para lo que tendremos que elegir la matriz de covarianzas de los efectos aleatorios:
$$
\begin{cases}
H_0 : \Omega = 
\begin{pmatrix}
\sigma^2_{\beta_0} & 0 \\
0 & \sigma^2_{\beta_1}
\end{pmatrix} \\
H_1 : \Omega = 
\begin{pmatrix}
\sigma^2_{\beta_0} & \sigma_{\beta_0 \beta_1} \\
\sigma_{\beta_0 \beta_1} & \sigma^2_{\beta_1}
\end{pmatrix}
\end{cases}
$$
En este caso, utilizaremos un test de razón de verosimilitudes para escoger la estructura de covarianzas de los efectos aleatorios y de sus errores. Para ello, hay que tener en cuenta que los modelos estén **anidados**, es decir, que la matriz de covarianzas de los residuos de un modelo se expresen como un caso particular de la matriz de covarianzas de los residuos del otro modelo.

Una vez hemos terminado con los efectos aleatorios, procedemos a determinar la significación de los efectos fijos a través de dos métodos. Si queremos testear un sólo parámetro, utilizaremos el test de Wald en el que:
$$
\begin{cases}
H_0 : \beta_1 = 0 \\
H_1 : \beta_1 \neq 0
\end{cases}
$$
En caso de querer testear más de un parámetro, utilizaremos un test de razón de verosimilitudes:
$$
\begin{cases}
H_0 : \beta_1 = \beta_2 = 0 \\
H_1 : \text{alguno diferente de 0}
\end{cases}
$$
Una vez hemos definido ya nuestro modelo, tenemos que realizar su validación a través de comprobar que se cumplen las asunciones sobre los residuos; al igual que hacíamos con Regresión Lineal Simple. Para poder asumir que el modelo es correcto, en el gráfico **residuos estandarizados vs valores predichos**, debería de aparecer una especie de nube de puntos en los que no haya ningún patrón ni ninguna tendencia aparente; mientras que en el **QQ-plot**, si los residuos se encuentran alrededor de la diagonal sin seguir tampoco ningún patrón, podremos asumir que los residuos tienen normalidad. Para validar lod efectos aleatorios, podemos utilizar **Empirical Bayes Estimates** en lugar de asumir su normalidad.


## Modelos Lineales Generalizados (GLM)
Los Modelos Lineales Generalizados son una generalización de los modelos lineales para una variable respuesta perteneciente a la familia exponencial, en la que tenemos una función link que describe como la media de la variable respuesta y la combinación lineal de variables explicativas están relacionadas.

La **familia exponencial** suele tener esta forma:
$$
f(y \mid \theta, \phi) = \exp \left[ \frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi) \right]
$$
En esta ecuación, $\theta$ es el **parámetro canónico** y representa la posición (location); mientras que $\phi$ es el **parámetro de dispersión** y representa la escala (scale). De la misma forma, $a$, $b$ y $c$ representan diferentes miembros de la familia exponencial. En función del parámetro de dispersión, podemos distinguir entre familias exponenciales de **un** parámetro, y familias exponenciales de **dos** parámetros.

Para determinar si un modelo está basado en un único parámetro $\theta$, tenemos que poder escribir su función de probabilidad de la siguiente forma:
$$
f(y; \theta) = e^{[a(y)b(\theta) + c(\theta) + d(y)]}
$$
Si el conjunto de posibles valores de entrada no depende de $\theta$, la familia exponencial será de un parámetro. Como familias exponenciales de un parámetro, tenemos las distribuciones de Poisson y la Binomial. Vamos a demostrar que la distribución de Poisson es, en efecto, una familia exponencial de un parámetro.

Para ello, aplicando propiedades logarítmicas, podemos definir la distribución de Poisson como:
$$
P(Y = y) = e^{-\lambda} e^{y \log \lambda} e^{-\log(y!)} \\
= e^{y \log \lambda - \lambda - \log(y!)}
$$
Si comparamos esta función de masa de probabilidad con la fnción de probabilidad general para familias con un único parámetro, podemos ver que:
$$
\begin{aligned}
a(y) &= y \\
b(\theta) &= \log(\lambda) \\
c(\theta) &= -\lambda \\
d(y) &= -\log(y!)
\end{aligned}
$$
La función $b(\theta)$ es lo que denominamos **link canónico**, una función que nos permite modelar como una función lineal de variables explicativas.

Como familias exponenciales de dos parámetros, tenemos la distribución Gamma y la Normal. De forma parecida a la anterior, podemos demostrar que la distribución Normal es una familia exponencial de dos parámetros.

Podemos definir la función de densidad de una distribución Normal como:
$$
f(y | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(y - \mu)^2}{2\sigma^2} \right)
$$
Si separamos términos y los escribimos como términos logarítmicos, tenemos que: 
$$
f(y | \mu, \sigma^2) = \exp \left( y \cdot \frac{\mu}{\sigma^2} - \frac{y^2}{2\sigma^2} + \left( -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2) \right) \right)
$$
Si comparamos esta función de densidad con la forma general de la familia exponencial, podemos ver que:
$$
\begin{aligned}
a(y) = y\\
b(\mu, \sigma^2) = \frac{\mu}{\sigma^2}\\
c(\mu, \sigma^2) = -\frac{\mu^2}{2\sigma^2} - \frac{1}{2} \log(2\pi\sigma^2)\\
d(y, \sigma^2) = -\frac{y^2}{2\sigma^2}\\
\end{aligned}
$$
Por lo tanto, demostramos que la distribución normal también pertenece a la familia exponencial, pero con una peculiaridad respecto a la distribución de Poisson: es una familia exponencial de dos parámetros, la media $\mu$ y la varianza $\sigma^2$. En este caso, el término $b(\mu, \sigma^2)$ es el **link canónico** que conecta las variables explicativas con el modelo. 

En concreto, para los casos en los que la respuesta no es normal, la ecuación del modelo es la siguiente:
$$
g\left(E(y_{ij} \mid x_{ijk}, \beta_{0i}, \dots, \beta_{Ki})\right) = \beta_{0i} + \sum_{k=1}^{K} \beta_{ki}x_{ijk}
$$
Donde g es la función link y, pese a que puede parecerse mucho a la función para modelos LMM, tienen algunas diferencias, como que en el primer miembro tenemos el link del valor esperado en vez de la variable respuesta, y en el segundo miembro no se cuenta con los errores; por lo que no existe una matriz de correlaciones de los residuos. De esta forma, ya hemos **generalizado** nuestro modelo para manejar variables respuesta que no siguen una distribución normal. A través de esta generalización, somos capaces de escribir la función de masa o densidad de probabilidad de distintas distribuciones para poder modelar el link canónico como función lineal de las variables predictoras.